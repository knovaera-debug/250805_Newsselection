import os
import time
import re
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

# =========================
# è¨­å®šé …ç›®
# =========================
SPREADSHEET_ID = '1ZqRekcKkUUoVxZuO8hrWRWwTauyEk8kD_NmV5IZy02w'
INPUT_SHEET_NAME = 'input'
BASE_SHEET_NAME = 'Base'
TODAY_SHEET_NAME = datetime.now().strftime("%y%m%d")

# =========================
# Google Sheetsèªè¨¼
# =========================
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
client = gspread.authorize(creds)
sheet = client.open_by_key(SPREADSHEET_ID)

# =========================
# WebDriverã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# =========================
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(options=options)

# =========================
# URLãƒªã‚¹ãƒˆã®å–å¾—
# =========================
input_ws = sheet.worksheet(INPUT_SHEET_NAME)
urls = input_ws.col_values(3)[1:]  # C2ä»¥é™

# =========================
# æ–°ã—ã„æ—¥ä»˜ã‚·ãƒ¼ãƒˆã‚’ä½œæˆï¼ˆBaseã‚’ã‚³ãƒ”ãƒ¼ï¼‰
# =========================
try:
    sheet.duplicate_sheet(source_sheet_id=sheet.worksheet(BASE_SHEET_NAME).id, new_sheet_name=TODAY_SHEET_NAME)
except Exception as e:
    print("â—ã‚·ãƒ¼ãƒˆè¤‡è£½ã‚¨ãƒ©ãƒ¼:", e)

out_ws = sheet.worksheet(TODAY_SHEET_NAME)

# =========================
# æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—å‡¦ç†
# =========================
def extract_article_and_comments(url):
    driver.get(url)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_tag = soup.find('title')
    title = title_tag.text.strip() if title_tag else 'ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰'

    # æœ¬æ–‡
    body_area = soup.find('article') or soup.find('div', class_='article_body')
    body = body_area.get_text(separator='\n').strip() if body_area else 'ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰'

    # ã‚³ãƒ¡ãƒ³ãƒˆ
    comments = []
    comment_divs = soup.find_all('p', class_='sc-169yn8p-10 hYFULX')
    for div in comment_divs:
        text = div.text.strip()
        if text:
            comments.append(text)

    return title, body, comments

# =========================
# å„URLã«å¯¾ã™ã‚‹å‡¦ç†
# =========================
for idx, url in enumerate(urls):
    if not url.startswith("http"):
        continue

    print(f"ğŸ” {idx+1}: {url}")
    try:
        title, body, comments = extract_article_and_comments(url)

        # å‡ºåŠ›
        out_ws.update(f'A{2 + idx}', str(idx + 1))  # No
        out_ws.update(f'B{2 + idx}', title)
        out_ws.update(f'C{2 + idx}', url)
        out_ws.update(f'D{2 + idx}', body)
        out_ws.update(f'E{2 + idx}', '\n'.join(comments[:30]))  # æœ€å¤§30ä»¶
        out_ws.update(f'F{2 + idx}', len(comments))  # ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°

        # ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°ã‚’inputã‚·ãƒ¼ãƒˆã®Fåˆ—ã«ã‚‚åæ˜ 
        input_ws.update(f'F{2 + idx}', len(comments))

    except Exception as e:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {url} â†’ {e}")
        out_ws.update(f'B{2 + idx}', 'ï¼ˆå–å¾—å¤±æ•—ï¼‰')
        input_ws.update(f'F{2 + idx}', 'NG')

driver.quit()
print("âœ… å®Œäº†")
